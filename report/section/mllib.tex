\section*{MLlib}

This sections presents an overview of how Spark's MLlib library handles the training of machine learning models, specifically those generated by the Stochastic Gradient Descent (SGD) algorithm (i.e. SVM, Logistic
Regression, Linear Regression). This MLlib approach serves as a baseline for the next sections where we discuss the use of parameter server.

Under MLlib a Gradient Descent training procedure runs as follows:
\begin{enumerate}
  \item Data is loaded into an RDD.
  \item For each iteration:
  \begin{enumerate}[label*=\arabic*.]
    \item Spark driver broadcast weights to all workers.
    \item A treeAggregation operation is executed over the RDD. On each worker:
    \begin{enumerate}[label*=\arabic*.]
      \item (map function) Worker computes local gradient using broadcasted weights and local data
      \item (reduce function) There might be communication between workers for aggregating their local gradients
    \end{enumerate}
    \item (reduce function) Driver aggregates local gradients.
    \item Driver update weights
  \end{enumerate}
  \item Spark driver use parameters for making predictions.
\end{enumerate}

The following drawbacks is what motivates the use of Parameter Server:
\begin{itemize}
  \item There might be an additional overhead for running an RDD operation on each iteration.
  \item RDD disables the use of any asynchronous training procedure. RDD operation is a blocking operation: fast workers must wait for lower ones before continuing running.
  \item What if the model is too big to fit in memory? MLlib cannot handle very-large models.
\end{itemize}
